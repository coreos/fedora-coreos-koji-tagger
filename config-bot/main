#!/usr/bin/python3 -u

import os
import re
import sys
import json
import toml
import logging
import fnmatch
import aiohttp
import asyncio
import argparse
import tempfile
import subprocess

# Set local logging
logging.basicConfig(
    format="%(asctime)s %(levelname)s %(name)s - %(message)s",
    level=logging.INFO)


DEFAULT_CONFIG_FILE_PATH = "/etc/config-bot.toml"

git = None


def main():
    args = parse_args()
    cfg = load_config(args.config)

    global git
    git = Git(cfg['git'])

    loop = asyncio.get_event_loop()

    o = cfg.get('sync-build-lockfiles')
    if o is not None:
        loop.create_task(sync_build_lockfiles(o))

    o = cfg.get('promote-lockfiles')
    if o is not None:
        loop.create_task(promote_lockfiles(o))

    o = cfg.get('bump-lockfiles')
    if o is not None:
        loop.create_task(bump_lockfiles(o))

    o = cfg.get('propagate-files')
    if o is not None:
        loop.create_task(propagate_files(o))

    loop.run_forever()
    loop.close()


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', help="Path to TOML config file",
                        default=DEFAULT_CONFIG_FILE_PATH)
    return parser.parse_args()


def load_config(fn):
    with open(fn) as f:
        return toml.load(f)


async def sync_build_lockfiles(cfg):
    # this is the only mode we support right now
    assert cfg['trigger']['mode'] == 'periodic'
    period = period_to_seconds(cfg['trigger']['period'])
    method = cfg['method']

    base_url = cfg['builds-base-url']

    # {stream -> buildid}
    last_build_ids = {}

    first = True
    while True:

        # for easier hacking; just act immediately on first iteration
        # this allows us to safely use `continue` afterwards
        if not first:
            logging.info("end sync_build_lockfiles")
            await asyncio.sleep(period)
        first = False
        logging.info("start sync_build_lockfiles")

        for stream in cfg['streams']:
            builds_json = f'{base_url}/{stream}/builds/builds.json'
            builds = json.loads(await http_fetch(builds_json))
            if builds is None:
                # problem fetching the json; just ignore and we'll retry
                continue
            elif len(builds['builds']) == 0:
                logging.error(f"Stream {stream} has no builds!")
                continue

            latest_build = builds['builds'][0]
            latest_build_id = latest_build['id']

            # is this a new build?
            if latest_build_id == last_build_ids.get(stream):
                continue

            synced = await do_sync(base_url, method, stream, latest_build)
            if synced:
                last_build_ids[stream] = latest_build_id


async def http_fetch(url):
    async with aiohttp.request(url=url, method='GET') as resp:
        if resp.status != 200:
            logging.error(f"Error fetching {url}: got {resp.status}")
            return None
        return await resp.read()


async def do_sync(base_url, method, stream, build):
    # we only support direct git pushes for now
    assert method == 'push'

    build_id = build["id"]
    build_dir = f'{base_url}/{stream}/builds/{build_id}'

    lockfiles = {}
    for arch in build['arches']:
        path = f'manifest-lock.generated.{arch}.json'
        lockfiles[path] = await http_fetch(f'{build_dir}/{arch}/{path}')
        if lockfiles[path] is None:
            # got an error while fetching, just leave and we'll retry later
            # should probably use a custom Exception for this
            return False

    async with git:
        git.checkout(stream)
        for path, data in lockfiles.items():
            with open(git.path(path), 'wb') as f:
                f.write(data)

        if git.has_diff():
            git.commit(f"lockfiles: import from build {build_id}",
                       lockfiles.keys())
            try:
                git.push(stream)
            except Exception as e:
                # this can happen if we raced against someone/something
                # else pushing to the ref and we're out of date; we'll retry
                logging.info(f"Got exception during push: {e}")
                return False

    return True


async def promote_lockfiles(cfg):
    # this is the only mode we support right now
    assert cfg['trigger']['mode'] == 'periodic'
    period = period_to_seconds(cfg['trigger']['period'])

    # we only support direct git pushes for now
    assert cfg['method'] == 'push'

    source_ref, target_ref = (cfg['source-ref'], cfg['target-ref'])

    last_source_ref_checksum = None

    first = True
    while True:

        if not first:
            logging.info("end promote_lockfiles")
            await asyncio.sleep(period)
        first = False
        logging.info("start promote_lockfiles")

        async with git:
            # is there a new commit?
            source_ref_checksum = git.rev_parse(source_ref)
            if last_source_ref_checksum == source_ref_checksum:
                continue

            git.checkout(target_ref)

            # get the list of lockfiles from the source ref
            all_files = git.cmd_output('ls-tree', source_ref,
                                       '--name-only').splitlines()
            locks = [f for f in all_files if
                     matches_patterns(f, ['manifest-lock.generated.*.json'])]

            if len(locks) == 0:
                logging.error(f"No lockfiles found in {source_ref}")
                last_source_ref_checksum = source_ref_checksum
                continue

            # bring it into the index
            git.cmd('checkout', source_ref, '--', *locks)

            # and rename it to the non-generated version
            for lock in locks:
                git.cmd('mv', '--force', lock, lock.replace('.generated', ''))

            if git.has_diff():
                git.commit(f"lockfiles: import from {source_ref}")
                try:
                    git.push(target_ref)
                except Exception as e:
                    logging.error(f"Got exception during push: {e}")
                    continue

            last_source_ref_checksum = source_ref_checksum


async def bump_lockfiles(cfg):
    # this is the only mode we support right now
    assert cfg['trigger']['mode'] == 'periodic'
    period = period_to_seconds(cfg['trigger']['period'])

    # we only support direct git pushes for now
    assert cfg['method'] == 'push'

    first = True
    while True:

        if not first:
            logging.info("end bump_lockfiles")
            await asyncio.sleep(period)
        first = False
        logging.info("start bump_lockfiles")

        async with git:
            for ref in cfg['refs']:
                git.checkout(ref)
                with Cosa(git.path()) as cosa:
                    cosa.cmd("fetch", "--update-lockfile")

                if git.has_diff():
                    git.commit(f"lockfiles: bump to latest")
                    try:
                        git.push(ref)
                    except Exception as e:
                        logging.error(f"Got exception during push: {e}")
                        continue


async def propagate_files(cfg):
    # this is the only mode we support right now
    assert cfg['trigger']['mode'] == 'periodic'
    period = period_to_seconds(cfg['trigger']['period'])
    skip_files = cfg['skip-files']

    # we only support direct git pushes for now
    assert cfg['method'] == 'push'

    source_ref, target_refs = (cfg['source-ref'], cfg['target-refs'])

    last_source_ref_checksum = None

    first = True
    while True:

        if not first:
            logging.info("end propagate_files")
            await asyncio.sleep(period)
        first = False
        logging.info("start propagate_files")

        async with git:
            # is there a new commit?
            source_ref_checksum = git.rev_parse(source_ref)
            if last_source_ref_checksum == source_ref_checksum:
                continue

            # get the list of files from the source ref
            all_files = git.cmd_output('ls-tree', source_ref,
                                       '--name-only').splitlines()
            files_to_import = [f for f in all_files
                               if not matches_patterns(f, skip_files)]

            if len(files_to_import) == 0:
                logging.error(f"No files to propagate from {source_ref}")
                last_source_ref_checksum = source_ref_checksum
                continue

            for target_ref in target_refs:
                git.checkout(target_ref)

                # We want the same semantics as `rsync --delete`, i.e. delete
                # files in the target ref no longer in the source ref. We'll do
                # this by first deleting all the files, then importing the new
                # files.
                all_files = git.cmd_output('ls-tree', target_ref,
                                           '--name-only').splitlines()
                files_to_delete = [f for f in all_files
                                   if not matches_patterns(f, skip_files)]

                git.cmd('rm', '-r', '--', *files_to_delete)
                git.cmd('checkout', source_ref, '--', *files_to_import)
                commit = git.cmd_output('rev-parse', source_ref)

                if git.has_diff():
                    git.commit(f"tree: import changes from {source_ref} "
                               f"at {commit}")
                    try:
                        git.push(target_ref)
                    except Exception as e:
                        logging.error(f"Got exception during push: {e}")
                        break
            else:
                last_source_ref_checksum = source_ref_checksum


def matches_patterns(fn, patterns):
    for pattern in patterns:
        if fnmatch.fnmatch(fn, pattern):
            return True
    return False


# normalize to seconds
def period_to_seconds(s):
    assert re.match('^[0-9]+[smh]$', s)
    multi = {"s": 1, "m": 60, "h": 60*60}
    return int(s[:-1]) * multi[s[-1:]]


class Git:

    '''
        Convenience wrapper around shared git repo. To categorically rule out
        leftovers/workdirs left in funky states from various operations, and
        ensuring we always push what we mean, we use one main bare repo to
        share objects, but do all the work in transient worktrees.
    '''

    def __init__(self, cfg):
        self._git_bare = tempfile.TemporaryDirectory(prefix="config-bot.bare.")
        self._git_work = None

        self._git_env = dict(os.environ)
        self._git_env.update({
            "GIT_AUTHOR_NAME": cfg['author']['name'],
            "GIT_AUTHOR_EMAIL": cfg['author']['email'],
            "GIT_COMMITTER_NAME": cfg['author']['name'],
            "GIT_COMMITTER_EMAIL": cfg['author']['email'],
        })

        gh_owner = cfg['github']['repo']['owner']
        gh_name = cfg['github']['repo']['name']
        token_un = cfg['github']['token']['username']
        with open(cfg['github']['token']['path']) as f:
            token_pw = f.read().strip()

        url = f'https://{token_un}:{token_pw}@github.com/{gh_owner}/{gh_name}'
        self.cmd('clone', '--bare', url, '.')

        # we don't technically need a lock if we make sure that we never
        # `await` operations when using `with git`, though that's something I
        # can easily imagine regressing on
        self._lock = asyncio.Lock()

    def __del__(self):
        self._git_bare.cleanup()

    async def __aenter__(self):
        await self._lock.acquire()
        self.cmd('fetch', 'origin', '--prune', '+refs/heads/*:refs/heads/*')
        assert self._git_work is None
        d = tempfile.TemporaryDirectory(prefix="config-bot.work.")
        self.cmd('worktree', 'add', '--detach', d.name, 'HEAD')
        self._git_work = d

    async def __aexit__(self, exc_type, exc, tb):
        self._git_work.cleanup()
        self._git_work = None
        self.cmd('worktree', 'prune')
        self._lock.release()

    def cmd(self, *args):
        wd = self._git_work or self._git_bare
        subprocess.check_call(['git', *args], cwd=wd.name, env=self._git_env)

    def cmd_output(self, *args):
        wd = self._git_work or self._git_bare
        out = subprocess.check_output(['git', *args], cwd=wd.name,
                                      env=self._git_env)
        return out.strip().decode('utf-8')

    def rev_parse(self, ref):
        return self.cmd_output('rev-parse', ref)

    def checkout(self, ref):
        self.cmd('checkout', '--detach', ref)

    def commit(self, message, files=None):
        if files and len(files) > 0:
            self.cmd('add', *files)
        self.cmd('commit', '-m', message)

    def push(self, ref):
        self.cmd('push', 'origin', f'HEAD:{ref}')

    def path(self, file=None):
        wd = self._git_work or self._git_bare
        if file is None:
            return wd.name
        return os.path.join(wd.name, file)

    def has_diff(self):
        # use ls-files instead of `diff --exit-code` so new untracked files
        # also count as a "diff"
        out = self.cmd_output('ls-files', '--modified', '--others').strip()
        out.strip()
        if len(out) > 0:
            return True

        # but also check whether we have things staged
        out = self.cmd_output('diff', '--staged', '--name-only')
        out.strip()
        if len(out) > 0:
            return True

        return False


class Cosa:

    '''
        Convenience wrapper for creating and nuking temporary cosa workdirs.
    '''

    def __init__(self, src_config_path):
        self._src_config_path = src_config_path
        self._workdir = None

    def __enter__(self):
        d = tempfile.TemporaryDirectory(prefix="cosa.work.")
        self._workdir = d
        self.cmd("init", self._src_config_path)
        return self

    def __exit__(self, exc_type, exc, tb):
        assert self._workdir
        self._workdir.cleanup()
        self._workdir = None

    def cmd(self, *args):
        assert self._workdir
        subprocess.check_call(['cosa', *args], cwd=self._workdir.name)


if __name__ == "__main__":
    sys.exit(main())
